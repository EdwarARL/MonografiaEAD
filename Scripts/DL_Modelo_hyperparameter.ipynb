{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhU1jBLUJDVC"
      },
      "source": [
        "#### Cargas las librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MDR2MmlYJDVH"
      },
      "outputs": [],
      "source": [
        "# Load Libraries\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "from pathlib import Path\n",
        "import os\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras_tuner import HyperModel\n",
        "from keras_tuner.tuners import Hyperband\n",
        "from regex import B\n",
        "from tensorflow.keras.optimizers import Adam, Adadelta\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "from evaluation._mae import MAE\n",
        "from evaluation._smape import sMAPE\n",
        "from evaluation._mape import MAPE\n",
        "from evaluation._rmse import RMSE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtHhixpaJDVJ"
      },
      "source": [
        "#### Cargar dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OpazuTyBJDVJ"
      },
      "outputs": [],
      "source": [
        "s_path=Path.cwd()\n",
        "s_prt_path=s_path.parent\n",
        "file=os.path.join(s_prt_path,'datasets/XM_H_sinOfe.csv')\n",
        "df_Data= pd.read_csv(file, index_col=0)\n",
        "# df_Data.reset_index(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IrsowmQbJDVK",
        "outputId": "2c824826-638c-4e37-99f0-10b594d0bbe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 52584 entries, 01/01/2013 0:00 to 31/12/2018 23:00\n",
            "Data columns (total 7 columns):\n",
            " #   Column     Non-Null Count  Dtype  \n",
            "---  ------     --------------  -----  \n",
            " 0   PrecioB    52584 non-null  float64\n",
            " 1   Demanda    52584 non-null  float64\n",
            " 2   DispTer    52584 non-null  int64  \n",
            " 3   DispNoTer  52584 non-null  int64  \n",
            " 4   VolUtil    52584 non-null  int64  \n",
            " 5   Aportes    52584 non-null  int64  \n",
            " 6   day_cls    52584 non-null  int64  \n",
            "dtypes: float64(2), int64(5)\n",
            "memory usage: 3.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df_Data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5xqZLEaJDVL"
      },
      "source": [
        "#### Funciones necesarias para las ejecuciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m76oH7LMJDVM"
      },
      "source": [
        "##### División de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "IYlaPjpHJDVM"
      },
      "outputs": [],
      "source": [
        "# Dividir los datos en train, validación y test\n",
        "def split_data(data, indexTotal, train_fraq, test_len=8760):\n",
        "\n",
        "    #slice the last year of data for testing 1 year has 8760 hours\n",
        "    test_slice = len(data)-test_len\n",
        "\n",
        "    test_data = data[test_slice:]\n",
        "    indexTest=indexTotal[test_slice:]\n",
        "\n",
        "    train_val_data = data[:test_slice]\n",
        "    index_val_data=indexTotal[:test_slice]\n",
        "\n",
        "    #make train and validation from the remaining\n",
        "    train_size = int(len(train_val_data) * train_fraq)\n",
        "    \n",
        "    train_data = train_val_data[:train_size]\n",
        "    indexTrain=index_val_data[:train_size]\n",
        "\n",
        "    val_data = train_val_data[train_size:]\n",
        "    indexVal=index_val_data[train_size:]\n",
        "    \n",
        "    return train_data, val_data, test_data,train_val_data, indexTrain, indexVal, indexTest, index_val_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CMmR0SrJDVN"
      },
      "source": [
        "##### Construcción del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XBZddQjXJDVN"
      },
      "outputs": [],
      "source": [
        "# Crear los datos para entrar al modelo, definición de ventanas\n",
        "def window_dataset(data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=False, expand_dims=False):\n",
        "    \"\"\" Create a windowed tensorflow dataset\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    #create a window with n steps back plus the size of the prediction length\n",
        "    window = n_steps + n_horizon\n",
        "    \n",
        "    #expand dimensions to 3D to fit with LSTM inputs\n",
        "    #creat the inital tensor dataset\n",
        "    if expand_dims:\n",
        "        ds = tf.expand_dims(data, axis=-1)\n",
        "        ds = tf.data.Dataset.from_tensor_slices(ds)\n",
        "    else:\n",
        "        ds = tf.data.Dataset.from_tensor_slices(data)\n",
        "    \n",
        "    #create the window function shifting the data by the prediction length\n",
        "    ds = ds.window(window, shift=n_horizon, drop_remainder=True)\n",
        "    \n",
        "    #flatten the dataset and batch into the window size\n",
        "    ds = ds.flat_map(lambda x : x.batch(window))\n",
        "    #ds = ds.shuffle(shuffle_buffer)\n",
        "    \n",
        "    #create the supervised learning problem x and y and batch\n",
        "    if multi_var:\n",
        "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:, :1]))\n",
        "    else:\n",
        "        ds = ds.map(lambda x : (x[:-n_horizon], x[-n_horizon:]))\n",
        "    \n",
        "    ds = ds.batch(batch_size).prefetch(1)\n",
        "    \n",
        "    return ds\n",
        "\n",
        "# Construcción del dataset usando las ventanas\n",
        "def build_dataset(traindata,valdata,testdata,train_val_data,train_fraq=0.8, \n",
        "                  n_steps=24*30, \n",
        "                  n_horizon=24, \n",
        "                  batch_size=256, \n",
        "                  shuffle_buffer=500, \n",
        "                  expand_dims=False, \n",
        "                  multi_var=False):\n",
        "    \"\"\"If multi variate then first column is always the column from which the target is contstructed.\n",
        "    \"\"\"\n",
        "    \n",
        "    tf.random.set_seed(23)\n",
        "    \n",
        "    \n",
        "    train_ds = window_dataset(traindata, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
        "    val_ds = window_dataset(valdata, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
        "    test_ds = window_dataset(testdata, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
        "    train_val_ds = window_dataset(train_val_data, n_steps, n_horizon, batch_size, shuffle_buffer, multi_var=multi_var, expand_dims=expand_dims)\n",
        "    \n",
        "    print(f\"Prediction lookback (n_steps): {n_steps}\")\n",
        "    print(f\"Prediction horizon (n_horizon): {n_horizon}\")\n",
        "    print(f\"Batch Size: {batch_size}\")\n",
        "    print(\"Datasets:\")\n",
        "    print(train_ds.element_spec)\n",
        "    \n",
        "    return train_ds, val_ds, test_ds, train_val_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHJ8dz0zJDVO"
      },
      "source": [
        "##### Parámetros y ejecución"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "WfTh8vzIJDVO"
      },
      "outputs": [],
      "source": [
        "# Definición de parámetros para la ejecución del modelo\n",
        "def get_params(multivar=True):\n",
        "    lr=3e-4\n",
        "    n_steps=72\n",
        "    n_horizon=24\n",
        "    if multivar:\n",
        "        n_features=7\n",
        "    else:\n",
        "        n_features=1\n",
        "\n",
        "    return n_steps, n_horizon, n_features, lr\n",
        "\n",
        "# Almacenamiento de los datos de las ejecuciones\n",
        "def cfg_model_run(model, history, test_ds):\n",
        "    return {\"model\": model, \"history\" : history, \"test_ds\": test_ds}\n",
        "\n",
        "# Ejecuciones de los modelos con función de ventanado\n",
        "def run_model(traindata,valdata,testdata,train_val_data, model_name, model_func, model_configs, epochs):\n",
        "    \n",
        "    n_steps, n_horizon, n_features, lr = get_params(multivar=True)\n",
        "    \n",
        "    train_ds, val_ds, test_ds, train_val_ds = build_dataset(traindata,valdata,testdata,train_val_data,n_steps=n_steps, n_horizon=n_horizon, multi_var=True)\n",
        "\n",
        "    model = model_func(n_steps, n_horizon, n_features, lr=lr)\n",
        "\n",
        "    model_hist = model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n",
        "\n",
        "    model_configs[model_name] = cfg_model_run(model, model_hist, test_ds)\n",
        "    \n",
        "    return 'Finalizado'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux09kK6w39oB"
      },
      "source": [
        "#### Escalamiento de los datos y división del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N6r645lA39oC",
        "outputId": "a8d94522-b341-48f9-93ed-aa48bcee6a3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Multivarate Datasets\n",
            "Train Data Shape: (35059, 7)\n",
            "Val Data Shape: (8765, 7)\n",
            "Test Data Shape: (8760, 7)\n",
            "Train Val Data Shape: (43824, 7)\n",
            "Index Val Data Shape: (35059,)\n",
            "Nulls In Train False\n",
            "Nulls In Validation False\n",
            "Nulls In Test False\n",
            "Precio (52584, 1)\n",
            "Todo (52584, 7)\n"
          ]
        }
      ],
      "source": [
        "#scale\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scalerP=MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "vec_precio=scalerP.fit_transform(df_Data.values[:,0].reshape(-1,1))\n",
        "\n",
        "indexTotal=df_Data.index\n",
        "\n",
        "multivar_df = scaler.fit_transform(df_Data)\n",
        "train_multi, val_multi, test_multi,train_val_multi, indexTrain, indexVal, indexTest, index_val_data = split_data(multivar_df,indexTotal, train_fraq=0.8, test_len=8760)\n",
        "print(\"Multivarate Datasets\")\n",
        "print(f\"Train Data Shape: {train_multi.shape}\")\n",
        "print(f\"Val Data Shape: {val_multi.shape}\")\n",
        "print(f\"Test Data Shape: {test_multi.shape}\")\n",
        "print(f\"Train Val Data Shape: {train_val_data.shape}\")\n",
        "print(f\"Index Val Data Shape: {indexTrain.shape}\")\n",
        "print(f\"Nulls In Train {np.any(np.isnan(train_multi))}\")\n",
        "print(f\"Nulls In Validation {np.any(np.isnan(val_multi))}\")\n",
        "print(f\"Nulls In Test {np.any(np.isnan(test_multi))}\")\n",
        "print('Precio',vec_precio.shape)\n",
        "print('Todo',df_Data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f_t6J2J39oC"
      },
      "source": [
        "##### Obtener dataset para el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "WuXiniJ739oC",
        "outputId": "c79ef560-7878-4653-dc79-9a1bd1cf123e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction lookback (n_steps): 72\n",
            "Prediction horizon (n_horizon): 24\n",
            "Batch Size: 35059\n",
            "Datasets:\n",
            "(TensorSpec(shape=(None, None, 7), dtype=tf.float64, name=None), TensorSpec(shape=(None, None, 1), dtype=tf.float64, name=None))\n",
            "Example sample shapes\n",
            "x =  (1457, 72, 7)\n",
            "y =  (1457, 24, 1)\n"
          ]
        }
      ],
      "source": [
        "n_steps=72\n",
        "n_horizon=24\n",
        "\n",
        "train_ds, val_ds, test_ds, train_val_ds = build_dataset(train_multi, val_multi, test_multi,train_val_multi, n_steps=n_steps, n_horizon=n_horizon,batch_size=train_multi.shape[0],multi_var=True)\n",
        "print('Example sample shapes')\n",
        "for idx,(x,y) in enumerate(train_ds):\n",
        "    print(\"x = \", x.numpy().shape)\n",
        "    print(\"y = \", y.numpy().shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Example sample shapes\n",
            "x =  (1457, 72, 7)\n",
            "y =  (1457, 24, 1)\n",
            "x =  (362, 72, 7)\n",
            "y =  (362, 24, 1)\n",
            "x =  (362, 72, 7)\n",
            "y =  (362, 24, 1)\n",
            "x =  (1823, 72, 7)\n",
            "y =  (1823, 24, 1)\n"
          ]
        }
      ],
      "source": [
        "print('Example sample shapes')\n",
        "for idx,(x,y) in enumerate(train_ds):\n",
        "    print(\"x = \", x.numpy().shape)\n",
        "    print(\"y = \", y.numpy().shape)\n",
        "    Xtr= x.numpy()\n",
        "    Ytr=y.numpy()\n",
        "    break\n",
        "\n",
        "for idx,(x,y) in enumerate(val_ds):\n",
        "    print(\"x = \", x.numpy().shape)\n",
        "    print(\"y = \", y.numpy().shape)\n",
        "    Xval= x.numpy()\n",
        "    Yval=y.numpy()\n",
        "    break\n",
        "\n",
        "for idx,(x,y) in enumerate(test_ds):\n",
        "    print(\"x = \", x.numpy().shape)\n",
        "    print(\"y = \", y.numpy().shape)\n",
        "    Xts= x.numpy()\n",
        "    Yts=y.numpy()\n",
        "    break\n",
        "\n",
        "for idx,(x,y) in enumerate(train_val_ds):\n",
        "    print(\"x = \", x.numpy().shape)\n",
        "    print(\"y = \", y.numpy().shape)\n",
        "    Xtr_v= x.numpy()\n",
        "    Ytr_v=y.numpy()\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epyWGNGBJDVO"
      },
      "source": [
        "#### Modelos Optimizando Hyperparametros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k58r4m-JDVP"
      },
      "source": [
        "##### DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CARGA DEL MODELO "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DNNHyperModel(HyperModel):\n",
        "    def __init__(self, n_steps, n_horizon, n_features,lr):\n",
        "        self.n_steps = n_steps\n",
        "        self.n_horizon = n_horizon\n",
        "        self.n_features = n_features\n",
        "        self.lr = lr\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = tf.keras.models.Sequential()\n",
        "        model.add(tf.keras.layers.Flatten(input_shape=(self.n_steps, self.n_features)))\n",
        "\n",
        "        hp_seed =hp.Int('seed', min_value=1, max_value=1000, step=1)\n",
        "        tf.random.set_seed(hp_seed)\n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Choose an optimal value between 32-512\n",
        "        hp_units1 = hp.Int('units1', min_value=32, max_value=256, step=32)\n",
        "                # Choose an optimal value between 32-256\n",
        "        hp_units2 = hp.Int('units2', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_dropout1 =hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)\n",
        "\n",
        "        hp_dropout2 =hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Tune the activation function to use in the Dense layers\n",
        "        hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'leaky_relu'])\n",
        "\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'adadelta'])\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units1, activation=hp_activation))\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout1))\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units2, activation=hp_activation))\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout2))\n",
        "\n",
        "        model.add(tf.keras.layers.Dense(self.n_horizon))\n",
        "\n",
        "\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = Adam(learning_rate=self.lr)\n",
        "        else:\n",
        "            optimizer = Adadelta(learning_rate=self.lr)\n",
        "\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                    loss=hp.Choice('loss', values=['huber', 'mae']),\n",
        "                    metrics=['mae', 'mape'])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HYPERPARAMETRIZACIÓN DEL MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from c:\\Users\\sebas\\Documents\\ESP_DATOS\\SEMESTRE2\\MONOGRAFIA 2\\MonografiaEAD-main\\MonografiaEAD\\experimental_files\\dnn\\tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 8\n",
            "seed (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 1000, 'step': 1, 'sampling': 'linear'}\n",
            "units1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "units2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "dropout1 (Float)\n",
            "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
            "dropout2 (Float)\n",
            "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
            "activation (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'leaky_relu'], 'ordered': False}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [], 'values': ['adam', 'adadelta'], 'ordered': False}\n",
            "loss (Choice)\n",
            "{'default': 'huber', 'conditions': [], 'values': ['huber', 'mae'], 'ordered': False}\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
            "layer is 160. The optimal number of units in the second densely-connected\n",
            "layer is 160.The optimal activation function for the Dense layers\n",
            "is leaky_relu. The optimal optimizer for the model\n",
            "is adam. The optimal loss function for the model\n",
            "is mae.the optimal dropout for the model\n",
            "is 0.0.The optimal dropout for the model\n",
            "is 0.0.The optimal seed for the model\n",
            "is 522.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "\n",
        "hypermodel = DNNHyperModel(*get_params(multivar=True))\n",
        "\n",
        "s_prt_path=s_path.parent\n",
        "path=os.path.join(s_prt_path,'experimental_files')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='MAPE',\n",
        "    max_epochs=40,\n",
        "    directory=path,\n",
        "    project_name='dnn'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "for train_index, val_index in tscv.split(Xtr_v):\n",
        "    X_train, X_val = Xtr_v[train_index], Xtr_v[val_index]\n",
        "    y_train, y_val = Ytr_v[train_index], Ytr_v[val_index]\n",
        "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=20)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_dnn = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
        "layer is {best_hps_dnn.get('units1')}. The optimal number of units in the second densely-connected\n",
        "layer is {best_hps_dnn.get('units2')}.The optimal activation function for the Dense layers\n",
        "is {best_hps_dnn.get('activation')}. The optimal optimizer for the model\n",
        "is {best_hps_dnn.get('optimizer')}. The optimal loss function for the model\n",
        "is {best_hps_dnn.get('loss')}.the optimal dropout for the model\n",
        "is {best_hps_dnn.get('dropout1')}.The optimal dropout for the model\n",
        "is {best_hps_dnn.get('dropout2')}.The optimal seed for the model\n",
        "is {best_hps_dnn.get('seed')}.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-HrDlKsJDVQ"
      },
      "source": [
        "##### LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CARGA DEL MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LSTMHyperModel(HyperModel):\n",
        "    def __init__(self, n_steps, n_horizon, n_features,lr):\n",
        "        self.n_steps = n_steps\n",
        "        self.n_horizon = n_horizon\n",
        "        self.n_features = n_features\n",
        "        self.lr = lr\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = tf.keras.models.Sequential()\n",
        "\n",
        "        hp_seed =hp.Int('seed', min_value=1, max_value=1000, step=1)\n",
        "        tf.random.set_seed(hp_seed)\n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Choose an optimal value between 32-256\n",
        "        hp_units1 = hp.Int('units1', min_value=32, max_value=256, step=32)\n",
        "                # Choose an optimal value between 32-256\n",
        "        hp_units2 = hp.Int('units2', min_value=32, max_value=256, step=32)\n",
        "                # Choose an optimal value between 32-256\n",
        "        hp_units3 = hp.Int('units3', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_dropout1 =hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)\n",
        "\n",
        "        hp_dropout2 =hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Tune the activation function to use in the Dense layers\n",
        "        hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'leaky_relu'])\n",
        "\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'adadelta'])\n",
        "\n",
        "        model.add(tf.keras.layers.LSTM(units=hp_units1, activation=hp_activation, input_shape=(self.n_steps, self.n_features), return_sequences=True))\n",
        "        model.add(tf.keras.layers.LSTM(units=hp_units2, activation=hp_activation, return_sequences=False))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout1))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units3, activation=hp_activation))\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout2))\n",
        "        model.add(tf.keras.layers.Dense(self.n_horizon))\n",
        "\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = Adam(learning_rate=self.lr)\n",
        "        else:\n",
        "            optimizer = Adadelta(learning_rate=self.lr)\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                    loss=hp.Choice('loss', values=['huber', 'mae']),\n",
        "                    metrics=['mae', 'mape'])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HYPERPARAMETRIZACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "UugTcyU2JDVQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 90 Complete [00h 00m 42s]\n",
            "mape: 152.1175537109375\n",
            "\n",
            "Best mape So Far: 38.29347610473633\n",
            "Total elapsed time: 00h 36m 16s\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first LSTM layer\n",
            "layer is 64. The optimal number of units in the second LSTM layer\n",
            "layer is 32. The optimal number of units in the third Dense layer\n",
            "layer is 160.The optimal activation function for layers\n",
            "is tanh. The optimal optimizer for the model\n",
            "is adam. The optimal loss function for the model\n",
            "is mae.the optimal dropout for the model\n",
            "is 0.0.The optimal dropout for the model\n",
            "is 0.0.The optimal seed for the model\n",
            "is 272.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hypermodel = LSTMHyperModel(*get_params(multivar=True))\n",
        "\n",
        "\n",
        "s_prt_path=s_path.parent\n",
        "path=os.path.join(s_prt_path,'experimental_files')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='mape',\n",
        "    max_epochs=40,\n",
        "    directory=path,\n",
        "    project_name='LSTM'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "for train_index, val_index in tscv.split(Xtr_v):\n",
        "    X_train, X_val = Xtr_v[train_index], Xtr_v[val_index]\n",
        "    y_train, y_val = Ytr_v[train_index], Ytr_v[val_index]\n",
        "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=20)\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_lstm =tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first LSTM layer\n",
        "layer is {best_hps_lstm.get('units1')}. The optimal number of units in the second LSTM layer\n",
        "layer is {best_hps_lstm.get('units2')}. The optimal number of units in the third Dense layer\n",
        "layer is {best_hps_lstm.get('units3')}.The optimal activation function for layers\n",
        "is {best_hps_lstm.get('activation')}. The optimal optimizer for the model\n",
        "is {best_hps_lstm.get('optimizer')}. The optimal loss function for the model\n",
        "is {best_hps_lstm.get('loss')}.the optimal dropout for the model\n",
        "is {best_hps_lstm.get('dropout1')}.The optimal dropout for the model\n",
        "is {best_hps_lstm.get('dropout2')}.The optimal seed for the model\n",
        "is {best_hps_lstm.get('seed')}.\n",
        "\"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuHPJhG6JDVQ"
      },
      "source": [
        "##### GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CARGA DEL MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GRUHyperModel(HyperModel):\n",
        "    def __init__(self, n_steps, n_horizon, n_features,lr):\n",
        "        self.n_steps = n_steps\n",
        "        self.n_horizon = n_horizon\n",
        "        self.n_features = n_features\n",
        "        self.lr = lr\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = tf.keras.models.Sequential()\n",
        "\n",
        "        hp_seed =hp.Int('seed', min_value=1, max_value=1000, step=1)\n",
        "        tf.random.set_seed(hp_seed)\n",
        "\n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Choose an optimal value between 32-256\n",
        "        hp_units1 = hp.Int('units1', min_value=32, max_value=256, step=32)\n",
        "                # Choose an optimal value between 32-256\n",
        "        hp_units2 = hp.Int('units2', min_value=32, max_value=256, step=32)\n",
        "                # Choose an optimal value between 32-256\n",
        "        hp_units3 = hp.Int('units3', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_dropout1 =hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)\n",
        "\n",
        "        hp_dropout2 =hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Tune the activation function to use in the Dense layers\n",
        "        hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'leaky_relu'])\n",
        "\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'adadelta'])\n",
        "\n",
        "        model.add(tf.keras.layers.GRU(units=hp_units1, activation=hp_activation, input_shape=(self.n_steps, self.n_features), return_sequences=True))\n",
        "        model.add(tf.keras.layers.GRU(units=hp_units2, activation=hp_activation, return_sequences=False))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout1))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units3, activation=hp_activation))\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout2))\n",
        "        model.add(tf.keras.layers.Dense(self.n_horizon))\n",
        "\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = Adam(learning_rate=self.lr)\n",
        "        else:\n",
        "            optimizer = Adadelta(learning_rate=self.lr)\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                    loss=hp.Choice('loss', values=['huber', 'mae']),\n",
        "                    metrics=['mae', 'mape'])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "HYPERPARAMETRIZACIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_ruvL91EJDVR",
        "outputId": "e24d072d-11e3-428d-e58d-74c7bb6af3e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 90 Complete [00h 00m 44s]\n",
            "mape: 137.51544189453125\n",
            "\n",
            "Best mape So Far: 38.50728225708008\n",
            "Total elapsed time: 00h 34m 37s\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first GRU\n",
            "layer is 96. The optimal number of units in the second GRU\n",
            "layer is 32. The optimal number of units in the third Dense layer\n",
            "layer is 160.The optimal activation function for the layers\n",
            "is tanh. The optimal optimizer for the model\n",
            "is adam. The optimal loss function for the model\n",
            "is huber.the optimal dropout for the model\n",
            "is 0.0.The optimal dropout for the model\n",
            "is 0.0.The optimal seed for the model\n",
            "is 600.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hypermodel = GRUHyperModel(*get_params(multivar=True))\n",
        "\n",
        "s_prt_path=s_path.parent\n",
        "path=os.path.join(s_prt_path,'experimental_files')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='mape',\n",
        "    max_epochs=40,\n",
        "    directory=path,\n",
        "    project_name='GRU'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "for train_index, val_index in tscv.split(Xtr_v):\n",
        "    X_train, X_val = Xtr_v[train_index], Xtr_v[val_index]\n",
        "    y_train, y_val = Ytr_v[train_index], Ytr_v[val_index]\n",
        "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=20)\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_gru=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first GRU\n",
        "layer is {best_hps_gru.get('units1')}. The optimal number of units in the second GRU\n",
        "layer is {best_hps_gru.get('units2')}. The optimal number of units in the third Dense layer\n",
        "layer is {best_hps_gru.get('units3')}.The optimal activation function for the layers\n",
        "is {best_hps_gru.get('activation')}. The optimal optimizer for the model\n",
        "is {best_hps_gru.get('optimizer')}. The optimal loss function for the model\n",
        "is {best_hps_gru.get('loss')}.the optimal dropout for the model\n",
        "is {best_hps_gru.get('dropout1')}.The optimal dropout for the model\n",
        "is {best_hps_gru.get('dropout2')}.The optimal seed for the model\n",
        "is {best_hps_gru.get('seed')}.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RESULTADOS DE MEJOR HYPERPARAMETRIZACIÓN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_4m2c-sJDVR"
      },
      "source": [
        "#### CNN_LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from torch import seed\n",
        "\n",
        "\n",
        "class LSTMCNNHyperModel(HyperModel):\n",
        "    def __init__(self, n_steps, n_horizon, n_features,lr):\n",
        "        self.n_steps = n_steps\n",
        "        self.n_horizon = n_horizon\n",
        "        self.n_features = n_features\n",
        "        self.lr = lr\n",
        "\n",
        "    def build(self, hp):\n",
        "        model = tf.keras.models.Sequential()\n",
        "\n",
        "        hp_seed =hp.Int('seed', min_value=1, max_value=1000, step=1)\n",
        "        tf.random.set_seed(hp_seed)\n",
        "        # Tune the number of units in the first Dense layer\n",
        "        # Choose an optimal value between 32-256\n",
        "        hp_units1 = hp.Int('units1', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_units2 = hp.Int('units2', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_units3 = hp.Int('units3', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_filter1=hp.Int('filters1', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_filter2=hp.Int('filters2', min_value=32, max_value=256, step=32)\n",
        "\n",
        "        hp_dropout1 =hp.Float('dropout1', min_value=0.0, max_value=0.5, step=0.1)\n",
        "\n",
        "        hp_dropout2 =hp.Float('dropout2', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        # Tune the activation function to use in the Dense layers\n",
        "        hp_activation = hp.Choice('activation', values=['relu', 'tanh', 'leaky_relu'])\n",
        "\n",
        "        optimizer = hp.Choice('optimizer', values=['adam', 'adadelta'])\n",
        "\n",
        "        model.add(tf.keras.layers.Conv1D(filters=hp_filter1, kernel_size=6, activation=hp_activation, input_shape=(self.n_steps,self.n_features)))\n",
        "        model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "        model.add(tf.keras.layers.Conv1D(filters=hp_filter2, kernel_size=3, activation=hp_activation))\n",
        "        model.add(tf.keras.layers.MaxPooling1D(2))\n",
        "        model.add(tf.keras.layers.LSTM(units=hp_units1, activation=hp_activation, return_sequences=True))\n",
        "        model.add(tf.keras.layers.LSTM(units=hp_units2, activation=hp_activation, return_sequences=False))\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout1))\n",
        "        model.add(tf.keras.layers.Dense(units=hp_units3, activation=hp_activation))\n",
        "        model.add(tf.keras.layers.Dropout(hp_dropout2))\n",
        "        model.add(tf.keras.layers.Dense(self.n_horizon))\n",
        "\n",
        "        if optimizer == 'adam':\n",
        "            optimizer = Adam(learning_rate=self.lr)\n",
        "        else:\n",
        "            optimizer = Adadelta(learning_rate=self.lr)\n",
        "\n",
        "        model.compile(optimizer=optimizer,\n",
        "                    loss=hp.Choice('loss', values=['huber', 'mae']),\n",
        "                    metrics=['mae', 'mape'])\n",
        "\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZs2G6zEJDVR",
        "outputId": "8507dc37-13b3-4c94-ba55-74ed57d26813"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reloading Tuner from c:\\Users\\sebas\\Documents\\ESP_DATOS\\SEMESTRE2\\MONOGRAFIA 2\\MonografiaEAD-main\\MonografiaEAD\\experimental_files\\CNN-LSTM\\tuner0.json\n",
            "Search space summary\n",
            "Default search space size: 11\n",
            "seed (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 1, 'max_value': 1000, 'step': 1, 'sampling': 'linear'}\n",
            "units1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "units2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "units3 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "filters1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "filters2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 256, 'step': 32, 'sampling': 'linear'}\n",
            "dropout1 (Float)\n",
            "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
            "dropout2 (Float)\n",
            "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.5, 'step': 0.1, 'sampling': 'linear'}\n",
            "activation (Choice)\n",
            "{'default': 'relu', 'conditions': [], 'values': ['relu', 'tanh', 'leaky_relu'], 'ordered': False}\n",
            "optimizer (Choice)\n",
            "{'default': 'adam', 'conditions': [], 'values': ['adam', 'adadelta'], 'ordered': False}\n",
            "loss (Choice)\n",
            "{'default': 'huber', 'conditions': [], 'values': ['huber', 'mae'], 'ordered': False}\n",
            "\n",
            "The hyperparameter search is complete. The optimal number of units in the first LSTM\n",
            "layer is 160. The optimal number of units in the second LSTM\n",
            "layer is 192. The optimal number of units in the third Dense layer\n",
            "layer is 192.The optimal activation function for the layers\n",
            "is tanh. The optimal optimizer for the model\n",
            "is adam. The optimal loss function for the model\n",
            "is huber.the optimal dropout for the model\n",
            "is 0.0.The optimal dropout for the model\n",
            "is 0.0.The optimal seed for the model\n",
            "is 218.The optimal filter for the model first CONV1D layer\n",
            "is 192.The optimal filter for the model second CONV1D layer\n",
            "is 32.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "hypermodel = LSTMCNNHyperModel(*get_params(multivar=True))\n",
        "\n",
        "s_prt_path=s_path.parent\n",
        "path=os.path.join(s_prt_path,'experimental_files')\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "tuner = Hyperband(\n",
        "    hypermodel,\n",
        "    objective='mape',\n",
        "    max_epochs=40,\n",
        "    directory=path,\n",
        "    project_name='CNN-LSTM'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "\n",
        "for train_index, val_index in tscv.split(Xtr_v):\n",
        "    X_train, X_val = Xtr_v[train_index], Xtr_v[val_index]\n",
        "    y_train, y_val = Ytr_v[train_index], Ytr_v[val_index]\n",
        "    tuner.search(X_train, y_train, validation_data=(X_val, y_val), epochs=20)\n",
        "\n",
        "\n",
        "# Get the optimal hyperparameters\n",
        "best_hps_lstm_cnn=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "The hyperparameter search is complete. The optimal number of units in the first LSTM\n",
        "layer is {best_hps_lstm_cnn.get('units1')}. The optimal number of units in the second LSTM\n",
        "layer is {best_hps_lstm_cnn.get('units2')}. The optimal number of units in the third Dense layer\n",
        "layer is {best_hps_lstm_cnn.get('units3')}.The optimal activation function for the layers\n",
        "is {best_hps_lstm_cnn.get('activation')}. The optimal optimizer for the model\n",
        "is {best_hps_lstm_cnn.get('optimizer')}. The optimal loss function for the model\n",
        "is {best_hps_lstm_cnn.get('loss')}.the optimal dropout for the model\n",
        "is {best_hps_lstm_cnn.get('dropout1')}.The optimal dropout for the model\n",
        "is {best_hps_lstm_cnn.get('dropout2')}.The optimal seed for the model\n",
        "is {best_hps_lstm_cnn.get('seed')}.The optimal filter for the model first CONV1D layer\n",
        "is {best_hps_lstm_cnn.get('filters1')}.The optimal filter for the model second CONV1D layer\n",
        "is {best_hps_lstm_cnn.get('filters2')}.\n",
        "\"\"\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0k58r4m-JDVP",
        "l-HrDlKsJDVQ"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
